# Probability

We provide real-world data to learn "true" the value of latent variables.

Might we possibly provide sythentic, "ideal" data to learn ideal values? (Are those autoencoders?) Could the "ideal" data be the results of a vote (i.e. something we want to put into place?)

What if latent variables are policy values and data are real-world outcomes?

Outcomes <- Latent "Policy Values"? (topics)

Learn the distribution of "values" that caused given outcomes.

Learn the function mapping these values to something real-world. (i.e. labeling topics)

Learn the distribution of values that cause "ideal" outcomes.

Apply the map to this new distribution of latent values.

Words could be: number of different types of event?

Words could be: an ordering? With count being priority?

Can an ordering represent a distribution? Ordered Dirichlet?

A document is... a person, with priorities.

# Blockchain

Rules fixed at the start: Ethereum

Rules changeable from within: Legislative Democracy

# Representation

What is the relationship we want to represent?

In some sense, preference / differentiation.

We would like to take an object that was previously whole / undifferentiated, and divide it into two.

This object could be a concept: Vehicle -> Car | Ship

Or a political preference: Visa Limits ->  More | Less

A new, non-numeric data structure. Graphical?

X, Concept, SubConcept, SubConcept

(note: if we fix number of subconcepts to two, can we in theory keep subdividing to achieve arbitrary distinctions of concepts?)

Y, Option A, Option B, Preference
(note: same as above -- can we fix subdivision to two?)

