\subsection{Deployment to a BBVM Environment}

The aim of this work has been to develop methods of representing and analyzing preferences, to facilitate the large-scale coordination of individuals.
In particular, emphasis was placed on understanding the efficiency of analysis.
Efficient analysis would allow for the wider deployment of preference-resolution mechanisms, and their inclusion in a wider range of applications.

\bigskip

In particular, we believe there is opportunity to embed efficient preference-resolution mechanisms into applications deployed to blockchain-based virtual machines, such as Ethereum.
These platforms have the following consequential properties:

\begin{itemize}
	\item The platform is turing-complete, allowing for the performance of computations of arbitrary complexity.
	\item Computations carried out on the platform are redundant and immutable, making results very difficult to fabricate, and therefore highly legitimate.
	\item The platform is distributed, so the system can survive the loss of individual nodes.
\end{itemize}

One could imagine applications which continuously measure and analyze preference, and take autonomous action upon discovering sufficiently clear preference structure.
In general, the fragility of computer systems (susceptibility to hacking, for example), would likely make us reticent to devolve too much decision-making autonomy to these systems.
The resiliency and legitimacy of the BBVM platform, however, make these types of applications more feasible.

\bigskip

The large-scaled redundancy of computation on these platforms (all computations must be carried out by all nodes) means that any analysis run on these platforms will needs be of limited complexity.
This is a major constraint, and motivates the search for more efficient representations and analysis.
It is illustrative that the primary application of these technologies to date has been the creation of ``cryptocurrencies''.
The implementation of cryptocurrencies are computationally simple, requiring little more than constant-time ($O(1)$) arithmetic operations on floating-point numbers.

The promise here is great, however: were sufficiently efficient representations and analysis discovered, it would be possible to deploy applications which autonomously and legitimately coordinate activity  in a highly non-hierarchical way, without appealing to individual (and therefore susceptible to coercion or exploitation) leadership.
The computational power of these platforms is currently limited, but if the history of computing is any indication (in 1977 the Apple II had RAM measured in kilobytes), they will grow in power over time.
As such, methods of analysis too cumbersome to deploy today may find themselves surprisingly valuable in the years to come.

\subsection{Specialized Access Policies}

In our initial specification of preference graphs mechanics, we discussed the notion of an ``access policy'' governing the creation of preferences.
The simplest access policy would allow any entity to create any preference between any pair of items at any time.
This policy is highly open, but provides few safeguards against manipulation of the preference graph.
Innovations in access policies have the potential to limit the extent to which preference graphs can be manipulated.

One idea would be to incorporate a material cost to preference creation, disincentivizing  entities from creating frivolous preferences.
A related idea would be to tie access to some sort of secondary criteria, such as the ownership of shares in a venture.
In online environments, tying access to material conditions will likely be important in avoiding Sybil attacks \citep{danezis:2006}.

Returning to the earlier discussion of the role of price as a low-dimensional representation of economic information, we note that prices are set via an energy-intensive bargaining process, in which participants are well-incentivized to arrive at optimal prices.
It is easy to see how prices set at random would be of much less value; our historical experience with stock market irrationality strongly suggests that this is the case \citep{minsky}.
Consequently, we seem justified in concluding that the value of preference data will be tightly linked to the incentive structure accompanying their generation.

\subsection{Item Pruning and Active Learning}

As discussed earlier, given $n$ items there are $n\choose{2}$ possible pairs.
An attractive property of preference graphs is that the item set $V$ is dynamic, and new items can be created in real-time \citep{salganik:2015}.
A larger number of items, however, makes it more difficult to observe a sufficient number of preferences to accurately recover global preference structure.
As such, techniques for either 1) reducing the number of items to be considered, or 2) identifying and observing critical pairs, would help make preference resolution easier for a large number of items.

\subsubsection{Item Pruning}

We have already proposed one method for item pruning, the MMSB algorithm.
This algorithm takes in an arbitrary number of observations and learns $K << n$ prototypes which reflect a higher-level preference structure.
Possession of these prototypes and their relationships reduces the size of the problem in the following ways: 

\begin{itemize}
	\item If there are clear prototypes but complex preferences between prototypes, we can rephrase the question only in terms of the prototypes, easing the problem from complexity in $n$ to complexity in $K$ (``zooming out'').
	\item If there are clear preferences between prototypes, such that one prototype is universally preferred, then we can limit consideration only to the items associated with the winning prototype (``zooming in'').
\end{itemize}

This is only one example.
More techniques for item pruning would allow for the more efficient analysis of complex preference structure.

\subsubsection{Active Learning}

As said, $n$ items creates $n(n-1)/2$ possible pairs.
In this work, we have assumed that preferences will be observed at random, but in practice it is unlikely that all pairs will be of equal importance in discovering preference structure.
For example, if two items seems to be in general preferred over all other items, then we should prioritize learning the relative preference between those two items.
This prioritization is known as ``active learning'' and there exists a large literature on the topic (\cite{shahriari:2016}).

\subsection{Optimal Committee Discovery}

Throughout this work, we have implicitly assumed that larger numbers of entities leads to better, more legitimate decisions.
While this assumption is defensible when considering questions of an abstract or ethical nature (``what is our greatest value'', or ``how should we balance the budget''), this assumption is less defensible when considering technical questions (``how can we improve our energy infrastructure'', or ``which subcontractors should we hire to for this construction project'').
For questions requiring technical expertise, large numbers of non-specialized entities would have noisy and high-entropy (uninformative) preference structure.
It would seem that instead of including the largest number of entities, we should instead seek to assemble a subset of entities (a ``committee'') with the following three properties:

\begin{enumerate}
	\item All entities possess sufficient expertise to be able to make meaningful distinctions between items (nonrandom preference structure).
	\item The variation in preference among these entities adequately covers the variation in preference found in the larger community.
	\item The committee is small enough that meaningful social relations can be developed between the members \citep{dunbar:1992}.
\end{enumerate} 

We would expect preferences drawn from a committee with these properties would be more structured than preferences drawn from the larger community, and therefore easier to analyze and yielding more legitimate conclusions.

\bigskip

The question becomes that of discovering these committees.
One approach would be to pose a question to  all entities, and then consider only the subset of those entities whose preferences meet some criteria for minimal structure (such as having a low tournament entropy or high $||v_1||_2$).
Another approach would be to pose a general question to all entities, and then select those entities whose preferences for the general question meet some criteria, and then put to them the new, more specific question.
Grouping could be done using the per-entity $v_1$, using a clustering algorithm like K-Nearest Neighbors, or by looking at cosine similarity.

\subsection{Question Quality Assessment}

The notion of ``structure'' of preference makes it possible to empirically measure the ``quality'' of a question, with high-quality questions giving rise to highly structured preferences.
In an extreme case, a question consisting of nonsense letters would be expected to lead to completely unstructured answers: entities either abstaining or indicating preferences at random.
Such a question would be low-quality in that it does not give rise to structured preferences.
Following this reasoning, higher-quality questions are those which give rise to more well-structured preferences (however ``structure'' is defined in the context of the particular problem).

If we had two candidate questions $Q_1, Q_2$, and the same set of candidate answers, we could identify objectively the ``better'' question by seeing which question gave rise to more structured preferences.
It is interesting and encouraging to observe how the mechanics of preference graphs given in this work allow us to learn not only the relations between items, but the relations between questions.

\subsection{Preference Databases}

The simple and consistent representation of the pairwise preference lends itself well to the development of large, longitudinal preference databases.
One could imagine a general question (``what do you look for most in a partner'') being posed for an extended period of time, with answers being generated by many entities.
Assuming that each preference is accompanied by appropriate metadata, these preference databases would become a rich and versatile source of data, comparable to standard surveys such as the General Social Survey.

\subsection{Conclusions}

The German-American political theorist Hannah Arendt has written about the need for a ``public sphere'', in which there exist methods and structures to allow the achievement of collective freedom via the construction of a common world (\cite{dentreves:2016}).
In Arendt's view, the public sphere is artificial in that does not require grounding in notions of ``natural rights'', but is rather constructed and maintained somewhat arbitrarily by human beings.
Further, Arendt felt that political representation (via elected officials) limited the power of individuals and emphasized the distinction between the rulers and the ruled.
In the spirit of Arendt, we have attempted to lay groundwork for new mechanisms of direct political participation.

\bigskip

This work has explored the question of large-scale nonviolent coordination, taking as its starting point new powerful communication and computational technologies.
We found ourselves closely tracking the work of the social choice theorists, and incorporated ideas from machine learning to attempt to understand and solve those same problems.
We presented a general representation of preference, achieving the goal of formalizing subjectivity.
We then presented a number of algorithms, old and new, for analyzing these types of representations.

\bigskip

One might question the prudence of this line of research.
We appeal to our earlier assertion regarding computing technologies, as well as Arendt's notion of the constructed nature of the public sphere, to conclude that it is fully within our power to discover new ways of living and working together.
In a world stinging from the bitterness of inequality and rapidly losing faith in existing institutions of governance, this line of thinking has never been needed more.

\subsection{Acknowledgements}

We would like to thank Eleni Drinea for her thoughtful advising and valuable feedback on the presentation of a number of technical concepts, Avner May for his assistance in developing the Prototype algorithm, and Steve Bronder for his peer support.
We would also like to thank Steven Kronovet and Stephanie Lehman for graciously opening their 89th street apartment to the author, saving him a number of late-night and early-morning commutes from Bushwick. 